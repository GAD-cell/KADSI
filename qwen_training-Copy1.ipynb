{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eca8e7c-3e17-4b7b-b818-0b664ccb4b8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.49.0\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers==4.49.0)\n",
      "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.49.0)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.49.0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.49.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.49.0) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m207.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.31.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.49.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.6.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.21.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.6.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (4.13.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.6.0) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m268.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m272.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m246.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m249.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m208.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision==0.21.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision==0.21.0) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (768.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m205.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.2\n",
      "    Uninstalling torchvision-0.17.2:\n",
      "      Successfully uninstalled torchvision-0.17.2\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.2\n",
      "    Uninstalling torchaudio-2.2.2:\n",
      "      Successfully uninstalled torchaudio-2.2.2\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting xformers==0.0.27.post2\n",
      "  Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl==0.15.2\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: triton in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.2.0)\n",
      "Collecting cut_cross_entropy\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2025.5.7-py3-none-any.whl.metadata (8.0 kB)\n",
      "Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m197.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading unsloth_zoo-2025.5.7-py3-none-any.whl (138 kB)\n",
      "Installing collected packages: xformers, unsloth_zoo, trl, peft, cut_cross_entropy, bitsandbytes, accelerate\n",
      "Successfully installed accelerate-1.7.0 bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 peft-0.15.2 trl-0.15.2 unsloth_zoo-2025.5.7 xformers-0.0.27.post2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (5.29.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.31.2)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.13.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, propcache, multidict, hf_transfer, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2025.5.7 requires msgspec, which is not installed.\n",
      "unsloth-zoo 2025.5.7 requires tyro, which is not installed.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\n",
      "unsloth-zoo 2025.5.7 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
      "unsloth-zoo 2025.5.7 requires transformers!=4.47.0,==4.51.3, but you have transformers 4.49.0 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.6.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2025.3.0 hf_transfer-0.1.9 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 sentencepiece-0.2.0 xxhash-3.5.0 yarl-1.20.0\n",
      "Collecting unsloth==2025.4.1\n",
      "  Downloading unsloth-2025.4.1-py3-none-any.whl.metadata (46 kB)\n",
      "Downloading unsloth-2025.4.1-py3-none-any.whl (193 kB)\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.49.0\n",
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.27.post2 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth==2025.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8a2548-bb0a-482f-a130-305beef592f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfiglet\n",
      "  Downloading pyfiglet-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pyfiglet-1.0.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyfiglet\n",
      "Successfully installed pyfiglet-1.0.2\n"
     ]
    }
   ],
   "source": [
    "#!unzip VLM_GRPO-main.zip\n",
    "#!pip install -e VLM_GRPO-main\n",
    "#!pip install pyfiglet\n",
    "#!pip install -r VLM_GRPO-main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbe45d9-b991-4d26-96ac-1c03314a3895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "洶･ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      " _    ____    __  ___      __________  ____  ____         \n",
      "| |  / / /   /  |/  /     / ____/ __ \\/ __ \\/ __ \\        \n",
      "| | / / /   / /|_/ /_____/ / __/ /_/ / /_/ / / / /  ______\n",
      "| |/ / /___/ /  / /_____/ /_/ / _, _/ ____/ /_/ /  /_____/\n",
      "|___/_____/_/  /_/      \\____/_/ |_/_/    \\____/          \n",
      "                                                          \n",
      "    ____  ___  ______________  _______   ________\n",
      "   / __ \\/   |/_  __/ ____/ / / /  _/ | / / ____/\n",
      "  / /_/ / /| | / / / /   / /_/ // //  |/ / / __  \n",
      " / ____/ ___ |/ / / /___/ __  // // /|  / /_/ /  \n",
      "/_/   /_/  |_/_/  \\____/_/ /_/___/_/ |_/\\____/   \n",
      "                                                 \n",
      "   __  ___   _______ __    ____  ________  __\n",
      "  / / / / | / / ___// /   / __ \\/_  __/ / / /\n",
      " / / / /  |/ /\\__ \\/ /   / / / / / / / /_/ / \n",
      "/ /_/ / /|  /___/ / /___/ /_/ / / / / __  /  \n",
      "\\____/_/ |_//____/_____/\\____/ /_/ /_/ /_/   \n",
      "                                             \n",
      "\n",
      "Unsloth patched for VLMs GRPO training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.4.0+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.10.14 (you have 3.10.14)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "洶･ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.1: Fast Qwen2_5_Vl patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 21.951 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import vlmgrpo\n",
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "\n",
    "SFT_path=\"V2_GRPO\"\n",
    "#SFT_path=None\n",
    "if SFT_path != None :\n",
    "  model, tokenizer = FastVisionModel.from_pretrained(\n",
    "      SFT_path,\n",
    "      load_in_4bit=True,\n",
    "  )\n",
    "  #decommenter pour entrainer des nouveaux lora\n",
    "  #model=model.merge_and_unload()\n",
    "\n",
    "else:\n",
    "  model, tokenizer = FastVisionModel.from_pretrained(\n",
    "      \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n",
    "      load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "      use_gradient_checkpointing = False, # True or \"unsloth\" for long context\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a53dbbd-0503-4b76-93ef-aac4322cf6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = False, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071855ba-6e04-40fe-986c-1aaf4f7b223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os \n",
    "import pickle\n",
    "df = pd.read_csv('metadata_global.csv')\n",
    "\n",
    "with open('data.pkl','rb') as f:\n",
    "    dataset_CoT=pickle.load(f)\n",
    "\n",
    "df_CoT=pd.DataFrame(dataset_CoT)\n",
    "df_merged = pd.merge(df, df_CoT, left_on='isic_id', right_on='image_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4382841e-1701-4aa7-a749-865ab7f999c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<think>\"\n",
    "reasoning_end   = \"</think>\"\n",
    "solution_start  = \"<answer>\"\n",
    "solution_end    = \"</answer>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem. Think about the problem and provide your working out. Place it between {reasoning_start} and {reasoning_end}. Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "prompt1 = \"\"\"Instruction : You are a medical image analysis assistant specialized in dermatology. You are shown a high-resolution image of a skin lesion.\n",
    "\n",
    "Your task is to carefully examine the image and provide a **detailed reasoning** based on observable features such as shape, color, border irregularities, texture, pattern...\n",
    "\n",
    "After your reasoning, provide the **final diagnosis** by selecting **only one** of the following predefined categories:\n",
    "\n",
    "Benign melanocytic proliferations\n",
    "Benign - Other\n",
    "Malignant melanocytic proliferations (Melanoma)\n",
    "Malignant adnexal epithelial proliferations - Follicular\n",
    "Benign epidermal proliferations\n",
    "Indeterminate epidermal proliferations\n",
    "Indeterminate melanocytic proliferations\n",
    "Collision - Only benign proliferations\n",
    "Malignant epidermal proliferations\n",
    "Benign soft tissue proliferations - Fibro-histiocytic\n",
    "Benign soft tissue proliferations - Vascular\n",
    "Collision - At least one malignant proliferation\n",
    "Flat melanotic pigmentations - not melanocytic nevus\n",
    "Inflammatory or infectious diseases\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1bc98b6-727b-4e14-9979-41d51557a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def img_aug(img):\n",
    "  transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomRotation(degrees=10, fill=(0, 0, 0)),\n",
    "  ])\n",
    "\n",
    "  return transform(img)\n",
    "\n",
    "\n",
    "def process_sample(item):\n",
    "    image_id = item[\"isic_id\"]\n",
    "    skin_type = item[\"type\"]\n",
    "    image_path = f\"dataset/{skin_type}/{image_id}.jpg\"\n",
    "    image = Image.open(image_path)\n",
    "    metadata = {'age_approx':item['age_approx'],'anatom_site_general':item['anatom_site_general'],'sex':item['sex']}\n",
    "    if pd.isna(item['diagnosis_2']):\n",
    "        diagnosis = item['diagnosis_1']\n",
    "    else:\n",
    "        diagnosis = item['diagnosis_2'] \n",
    "        \n",
    "    content = [{\"type\": \"text\",\"text\": prompt1 + f\"you are provided with an image andthe metadata concerning the patient:\\n approximate age: {metadata['age_approx']}, lesion location: {metadata['anatom_site_general']}, patient sex: {metadata['sex']}. Be concise.\"\n",
    "}]\n",
    "    content.append({\"type\": \"image\", \"image\": img_aug(image).resize((224,224))})\n",
    "    output =  {\"messages\": [\n",
    "            {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": content\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": item[\"CoT\"]}\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "  }\n",
    "    return output\n",
    "\n",
    "data_list = df_merged.to_dict(orient=\"records\")\n",
    "\n",
    "train_dataset=data_list[:280]\n",
    "eval_dataset=data_list[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f482cada-9421-4f9b-a913-3c8c3efb5f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 280 | Num Epochs = 10 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 14,024,704/7,000,000,000 (0.20% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/80 23:12 < 07:28, 0.04 it/s, Epoch 6.69/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.635900</td>\n",
       "      <td>1.586091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.425027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.231422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth import FastVisionModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "import os\n",
    "num_workers=os.cpu_count()-1\n",
    "from transformers import EarlyStoppingCallback\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # Optional set GPU device ID\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "config = SFTConfig(\n",
    "        dataloader_num_workers = num_workers,\n",
    "        per_device_train_batch_size = 8 ,\n",
    "        per_device_eval_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        #max_steps = 1000,\n",
    "        num_train_epochs = 10, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_SFT\",\n",
    "        report_to = \"none\",\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"messages\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "\n",
    "\n",
    "        eval_steps = 20,\n",
    "        eval_strategy = \"steps\",\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 40,\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "trainer = SFTTrainer(\n",
    "    \n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model,tokenizer,formatting_func=process_sample),\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    args = config\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb6694d-5abe-44c5-a429-fd85c40ed212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"V1_SFT\")  # Local saving\n",
    "tokenizer.save_pretrained(\"V1_SFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4c9b3b-e132-4c41-9b51-cdd24e387d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 'ISIC_0085644',\n",
       " 'prompt': [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Instruction : You are a medical image analysis assistant specialized in dermatology. You are shown a high-resolution image of a skin lesion.\\n\\nYour task is to carefully examine the image and provide a **detailed reasoning** based on observable features such as shape, color, border irregularities, texture, pattern...\\n\\nAfter your reasoning, provide the **final diagnosis** by selecting **only one** of the following predefined categories:\\n\\nBenign melanocytic proliferations\\nBenign - Other\\nMalignant melanocytic proliferations (Melanoma)\\nMalignant adnexal epithelial proliferations - Follicular\\nBenign epidermal proliferations\\nIndeterminate epidermal proliferations\\nIndeterminate melanocytic proliferations\\nCollision - Only benign proliferations\\nMalignant epidermal proliferations\\nBenign soft tissue proliferations - Fibro-histiocytic\\nBenign soft tissue proliferations - Vascular\\nCollision - At least one malignant proliferation\\nFlat melanotic pigmentations - not melanocytic nevus\\nInflammatory or infectious diseasesInput an image and the metadata concerning the patient:\\n approximate age: 50.0, lesion location: lower extremity, patient sex: femaleYou are given a problem. Think about the problem and provide your working out. Place it between <think> and </think>. Then, provide your solution between <answer></answer>\\n Be concise and short'},\n",
       "    {'type': 'image'}]}],\n",
       " 'image': [<PIL.Image.Image image mode=RGB size=224x224>],\n",
       " 'answer': 'Benign melanocytic proliferations'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis=['diagnosis_1', 'diagnosis_2', 'diagnosis_3','diagnosis_4', 'diagnosis_5']\n",
    "\n",
    "class FormattedDataset():\n",
    "    def __init__(self, dataset, format_fn=None):\n",
    "        self.dataset = dataset\n",
    "        self.format_fn = format_fn\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset.iloc[idx]\n",
    "        output = self.format_fn(item)\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def img_aug(img):\n",
    "    pass\n",
    "def process_sample(item):\n",
    "    image_id = item[\"isic_id\"]\n",
    "    skin_type = item[\"type\"]\n",
    "    image_path = f\"dataset/{skin_type}/{image_id}.jpg\"\n",
    "    image = Image.open(image_path)\n",
    "    metadata = {'age_approx':item['age_approx'],'anatom_site_general':item['anatom_site_general'],'sex':item['sex']}\n",
    "\n",
    "    if pd.isna(item['diagnosis_2']):\n",
    "        diagnosis = item['diagnosis_1']\n",
    "    else:\n",
    "        diagnosis = item['diagnosis_2'] \n",
    "        \n",
    "    content = [{\"type\": \"text\",\"text\": prompt1 + f\"Input an image and the metadata concerning the patient:\\n approximate age: {metadata['age_approx']}, lesion location: {metadata['anatom_site_general']}, patient sex: {metadata['sex']}\"+system_prompt+\"\\n Be concise and short\"\n",
    "}]\n",
    "    content.append({\"type\":\"image\"})\n",
    "    output = {\n",
    "        \"image_id\":image_id,\n",
    "        \"prompt\":[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": content\n",
    "            }],\n",
    "        \"image\":[image.resize((224,224))],\n",
    "        \"answer\":diagnosis\n",
    "    }\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "dataset = FormattedDataset(dataset=df,format_fn=process_sample)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda8510a-e192-4fb4-8ef1-b4b5ddd4ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "solution_end_regex = r\"</answer>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "\n",
    "        # No need to reward <start_working_out> since we always prepend it!\n",
    "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    benin_labels = {\n",
    "        \"Benign melanocytic proliferations\",\n",
    "        \"Benign - Other\",\n",
    "        \"Benign epidermal proliferations\",\n",
    "        \"Benign soft tissue proliferations - Fibro-histiocytic\",\n",
    "        \"Benign soft tissue proliferations - Vascular\"\n",
    "    }\n",
    "    malin_labels = {\n",
    "        \"Malignant melanocytic proliferations (Melanoma)\",\n",
    "        \"Malignant adnexal epithelial proliferations - Follicular\",\n",
    "        \"Malignant epidermal proliferations\"\n",
    "    }\n",
    "\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "\n",
    "        if guess == true_answer:\n",
    "            score += 5.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        else:\n",
    "            # Bonus si guess et true_answer sont dans la mﾃｪme catﾃｩgorie (bﾃｩnin ou malin)\n",
    "            guess_benin = guess in benin_labels\n",
    "            guess_malin = guess in malin_labels\n",
    "            true_benin = true_answer in benin_labels\n",
    "            true_malin = true_answer in malin_labels\n",
    "\n",
    "            if (guess_benin and true_benin) or (guess_malin and true_malin):\n",
    "                score += 2.0\n",
    "            else:\n",
    "                # On essaye de calculer un ratio numﾃｩrique comme avant\n",
    "                try:\n",
    "                    ratio = float(guess) / float(true_answer)\n",
    "                    if 0.9 <= ratio <= 1.1:\n",
    "                        score += 2.0\n",
    "                    elif 0.8 <= ratio <= 1.2:\n",
    "                        score += 1.5\n",
    "                    else:\n",
    "                        score -= 2.5\n",
    "                except:\n",
    "                    score -= 4.5\n",
    "        scores.append(score)\n",
    "    return scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc430f-103e-48b3-8260-26c4e1c1083e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,514 | Num Epochs = 1 | Total steps = 2,628\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 14,024,704/7,000,000,000 (0.20% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0338\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.6481\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='179' max='2628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 179/2628 3:40:46 < 50:54:41, 0.01 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / match_format_exactly</th>\n",
       "      <th>rewards / match_format_approximately</th>\n",
       "      <th>rewards / check_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>2.488351</td>\n",
       "      <td>115.687500</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>-2.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>134.312500</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>115.562500</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.559401</td>\n",
       "      <td>138.187500</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.437500</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>122.500000</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>3.077106</td>\n",
       "      <td>135.625000</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.093750</td>\n",
       "      <td>3.272199</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>2.236621</td>\n",
       "      <td>164.187500</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.590727</td>\n",
       "      <td>159.687500</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-2.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.939978</td>\n",
       "      <td>172.062500</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-3.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>135.312500</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>144.625000</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-4.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>3.579785</td>\n",
       "      <td>148.687500</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>124.437500</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>179.250000</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>127.125000</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.288675</td>\n",
       "      <td>146.562500</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>118.437500</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>134.750000</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>139.625000</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.294920</td>\n",
       "      <td>136.250000</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>3.562170</td>\n",
       "      <td>159.500000</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>-1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>3.532041</td>\n",
       "      <td>129.500000</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.045825</td>\n",
       "      <td>133.750000</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.875000</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>2.108678</td>\n",
       "      <td>142.875000</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.309401</td>\n",
       "      <td>120.375000</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.045825</td>\n",
       "      <td>130.062500</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>139.625000</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.954785</td>\n",
       "      <td>149.437500</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>132.562500</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.590727</td>\n",
       "      <td>142.062500</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>142.812500</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.406250</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>131.312500</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>3.349638</td>\n",
       "      <td>126.375000</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130.937500</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.125000</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>142.250000</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>138.500000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>134.250000</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>130.625000</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>1.204021</td>\n",
       "      <td>125.562500</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>124.500000</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.057587</td>\n",
       "      <td>145.125000</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>130.625000</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>1.967201</td>\n",
       "      <td>129.312500</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>119.687500</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>1.327106</td>\n",
       "      <td>138.562500</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>140.625000</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.406250</td>\n",
       "      <td>2.057587</td>\n",
       "      <td>126.687500</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136.562500</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>151.937500</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>132.812500</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>123.125000</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.812500</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>2.621207</td>\n",
       "      <td>140.062500</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.812500</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.779701</td>\n",
       "      <td>127.875000</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>1.967201</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129.375000</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>159.500000</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>165.937500</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>146.187500</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>132.562500</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>124.687500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.812500</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>147.250000</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>143.312500</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>141.125000</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>124.875000</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>2.212287</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>144.250000</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>2.632250</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>-3.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>135.875000</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>1.967201</td>\n",
       "      <td>116.750000</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>2.897199</td>\n",
       "      <td>135.312500</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.968750</td>\n",
       "      <td>1.502478</td>\n",
       "      <td>137.250000</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>133.750000</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.093750</td>\n",
       "      <td>3.231806</td>\n",
       "      <td>132.500000</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.562500</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.154701</td>\n",
       "      <td>128.750000</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>140.312500</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.187500</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>2.344356</td>\n",
       "      <td>122.625000</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>134.562500</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>2.057587</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.404701</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-3.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.906250</td>\n",
       "      <td>3.108722</td>\n",
       "      <td>128.250000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>152.125000</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>149.375000</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.057587</td>\n",
       "      <td>118.187500</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>130.125000</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-4.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>2.342201</td>\n",
       "      <td>124.875000</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.636608</td>\n",
       "      <td>130.437500</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-1.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>128.937500</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.250000</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.781250</td>\n",
       "      <td>2.932587</td>\n",
       "      <td>139.500000</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.391521</td>\n",
       "      <td>134.062500</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-2.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>144.437500</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-4.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>2.252193</td>\n",
       "      <td>137.312500</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.187500</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.349306</td>\n",
       "      <td>133.562500</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132.875000</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.093750</td>\n",
       "      <td>1.951304</td>\n",
       "      <td>118.500000</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>2.649787</td>\n",
       "      <td>146.687500</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-2.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.371207</td>\n",
       "      <td>123.437500</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.093750</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>128.375000</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.057587</td>\n",
       "      <td>131.812500</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>4.365093</td>\n",
       "      <td>147.250000</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>141.562500</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.281250</td>\n",
       "      <td>2.731897</td>\n",
       "      <td>145.687500</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>133.062500</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.889606</td>\n",
       "      <td>124.812500</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.406250</td>\n",
       "      <td>2.108722</td>\n",
       "      <td>123.312500</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129.937500</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>124.187500</td>\n",
       "      <td>0.004375</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>1.766521</td>\n",
       "      <td>140.312500</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>3.349304</td>\n",
       "      <td>135.625000</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>2.178695</td>\n",
       "      <td>147.125000</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-3.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>1.750694</td>\n",
       "      <td>146.062500</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>2.313194</td>\n",
       "      <td>138.750000</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>2.685339</td>\n",
       "      <td>135.062500</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>138.875000</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>1.763041</td>\n",
       "      <td>108.937500</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>123.250000</td>\n",
       "      <td>0.012456</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>1.573817</td>\n",
       "      <td>112.062500</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>4.468750</td>\n",
       "      <td>2.077106</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>1.858325</td>\n",
       "      <td>122.687500</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.216506</td>\n",
       "      <td>127.937500</td>\n",
       "      <td>0.011460</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.968750</td>\n",
       "      <td>2.558707</td>\n",
       "      <td>135.062500</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>131.937500</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>144.750000</td>\n",
       "      <td>0.014391</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.593750</td>\n",
       "      <td>1.119780</td>\n",
       "      <td>116.125000</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>1.001951</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.485322</td>\n",
       "      <td>146.562500</td>\n",
       "      <td>0.016615</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.968750</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>0.015325</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.011741</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-1.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.906250</td>\n",
       "      <td>1.647199</td>\n",
       "      <td>137.500000</td>\n",
       "      <td>0.012620</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>130.562500</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.189451</td>\n",
       "      <td>139.375000</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>2.418332</td>\n",
       "      <td>134.875000</td>\n",
       "      <td>0.018577</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>6.468750</td>\n",
       "      <td>2.871901</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>3.119780</td>\n",
       "      <td>136.062500</td>\n",
       "      <td>0.014498</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>2.938194</td>\n",
       "      <td>137.125000</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>2.000694</td>\n",
       "      <td>145.250000</td>\n",
       "      <td>0.016957</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.781250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>138.062500</td>\n",
       "      <td>0.019732</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.938194</td>\n",
       "      <td>115.625000</td>\n",
       "      <td>0.020807</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.746207</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>3.049534</td>\n",
       "      <td>140.750000</td>\n",
       "      <td>0.024421</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>-2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>7.781250</td>\n",
       "      <td>1.734035</td>\n",
       "      <td>125.062500</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>1.563194</td>\n",
       "      <td>159.437500</td>\n",
       "      <td>0.017846</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.406250</td>\n",
       "      <td>-1.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>2.688888</td>\n",
       "      <td>125.812500</td>\n",
       "      <td>0.022697</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>132.375000</td>\n",
       "      <td>0.016191</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-4.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>1.750694</td>\n",
       "      <td>138.312500</td>\n",
       "      <td>0.020022</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>2.544397</td>\n",
       "      <td>140.062500</td>\n",
       "      <td>0.018126</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>1.218750</td>\n",
       "      <td>1.750694</td>\n",
       "      <td>137.937500</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.371207</td>\n",
       "      <td>126.125000</td>\n",
       "      <td>0.023220</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>7.562500</td>\n",
       "      <td>1.688194</td>\n",
       "      <td>120.750000</td>\n",
       "      <td>0.023467</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>2.373158</td>\n",
       "      <td>120.125000</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>5.968750</td>\n",
       "      <td>2.933707</td>\n",
       "      <td>125.750000</td>\n",
       "      <td>0.020578</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.216506</td>\n",
       "      <td>120.687500</td>\n",
       "      <td>0.020845</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>2.189451</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>3.968750</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>105.312500</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>-0.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.024581</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>129.562500</td>\n",
       "      <td>0.019367</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>2.307280</td>\n",
       "      <td>133.375000</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>5.656250</td>\n",
       "      <td>1.933707</td>\n",
       "      <td>120.062500</td>\n",
       "      <td>0.027110</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.156250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0530\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0632\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0609\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0610\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0001\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0316\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0255\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0558\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0001\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0372\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0526\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0621\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0335\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0696\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0278\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0502\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0213\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0321\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0401\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0452\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0493\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0659\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0691\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0705\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0001\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0269\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0336\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0442\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0442\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0400\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0400\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0325\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0325\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0280\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0300\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0527\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0454\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0514\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0559\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0685\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0363\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0605\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0361\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0361\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0322\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0483\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0484\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0484\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0377\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0376\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0305\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0469\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0516\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0596\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0460\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0460\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0323\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0323\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0303\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0553\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0327\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0327\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0421\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0614\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0631\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0789\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0330\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0376\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0582\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0348\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0348\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0346\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0365\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0368\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0489\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0350\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0350\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0342\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0370\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0370\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0351\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0352\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0377\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0515\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0664\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0792\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0488\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0301\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0301\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0397\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0566\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0375\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0375\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0433\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0559\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0365\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0365\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0468\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0657\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0339\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0655\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0351\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0588\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0339\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0316\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0480\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0306\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0306\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0325\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0550\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0416\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0356\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0641\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0002\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0326\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0268\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0548\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0369\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0368\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0011\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0403\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0466\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0466\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0004\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0464\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0563\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0637\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0012\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0498\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0644\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0477\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0646\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0457\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0576\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0436\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0436\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0339\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0652\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0010\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0017\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0403\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0613\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0014\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0469\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0014\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0030\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0005\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 2.3409\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0010\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0721\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0406\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0685\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0435\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0713\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0490\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0862\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0326\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0323\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0007\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0459\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0239\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0240\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0634\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0756\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0583\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0735\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0447\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0447\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0006\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0011\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0586\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0837\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0007\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0523\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0657\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0718\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0433\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0585\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0008\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0010\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0403\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0549\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0003\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0007\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0723\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0719\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0676\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0810\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0447\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0449\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0568\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0568\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0528\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0527\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0702\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0982\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0006\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0666\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0009\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0014\n",
      "[DEBUG] Loss: 0.0000\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0421\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0828\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0338\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0651\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0668\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0666\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0813\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0814\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0011\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0022\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0754\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0964\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0666\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0663\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0725\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0868\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0016\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0621\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0021\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0554\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0669\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0876\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0620\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0717\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0023\n",
      "[DEBUG] Loss: 0.0001\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0408\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0609\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0979\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0438\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0744\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0597\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1079\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0371\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0521\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0778\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0774\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0021\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0438\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0462\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0652\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0605\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0695\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0029\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0379\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0030\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0553\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0031\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0424\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0509\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0652\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0657\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0723\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0418\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0418\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0635\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0633\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0016\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0382\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0608\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0805\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0903\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0997\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0693\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1055\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0571\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0886\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0720\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0843\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0680\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0891\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0512\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0512\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0031\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0682\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0843\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1151\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0607\n",
      "[DEBUG] Loss: 0.0007\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1325\n",
      "[DEBUG] Loss: 0.0010\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0736\n",
      "[DEBUG] Loss: 0.0002\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1054\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0622\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0771\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0886\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1092\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0519\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0521\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0585\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0743\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0499\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0812\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0527\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0722\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0500\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0504\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0779\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1053\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0020\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0994\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0804\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1220\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0698\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0698\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0471\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0713\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0748\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1204\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0033\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0057\n",
      "[DEBUG] Loss: 0.0003\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0870\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0870\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0668\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0876\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0580\n",
      "[DEBUG] Loss: 0.0006\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.1133\n",
      "[DEBUG] Loss: 0.0005\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0975\n",
      "[DEBUG] Loss: 0.0004\n",
      "[DEBUG] Params with grad: 224 / 1081\n",
      "[DEBUG] Grad norm: 0.0980\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from vlmgrpo import VLMGRPOTrainer\n",
    "from trl import GRPOConfig\n",
    "\n",
    "if hasattr(model, \"_flag_for_generation\"):\n",
    "    model._flag_for_generation = False\n",
    "FastVisionModel.for_training(model)\n",
    "    \n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-5,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    bf16 = is_bf16_supported(),\n",
    "    fp16 = not is_bf16_supported(),\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = 512,\n",
    "    max_completion_length = 256,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    #max_steps = 250,\n",
    "    #save_steps = 250,\n",
    "    max_grad_norm = 0.3,\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = VLMGRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class=tokenizer, # MUST put unsloth processor here !\n",
    "    reward_processing_classes = tokenizer, #Here also\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        \n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "    grad_verbose=True\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072b557e-f1fa-477e-82a2-3e4c4ad7ff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"V1_GRPO\")  # Local saving\n",
    "tokenizer.save_pretrained(\"V1_GRPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83338c5-50ec-42f9-b6d3-992944099386",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=df.iloc[int(len(df)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7c9a039-86c9-4f44-b057-0d22be0b26db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0\n",
      "FN: 0 FP: 0 TN: 0 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 0 FP: 0 TN: 1 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 0 FP: 0 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 0 FP: 0 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 0 FP: 1 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 1 FP: 1 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 1 FP: 1 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 2 FP: 1 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 2 FP: 1 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 2 FP: 1 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 2 FP: 1 TN: 2 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 2 FP: 1 TN: 3 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 3 FP: 1 TN: 3 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 3 FP: 1 TN: 3 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 3 FP: 1 TN: 3 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 3 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 3 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 3 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 4 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 4 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 5 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 5 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 5 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 6 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 5 FP: 1 TN: 6 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 6 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 6 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 6 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 7 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 8 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 9 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 9 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 6 FP: 1 TN: 10 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 7 FP: 1 TN: 10 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 7 FP: 1 TN: 10 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 7 FP: 1 TN: 10 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 7 FP: 1 TN: 11 TP: 0\n",
      "accuracy: 0.0\n",
      "FN: 7 FP: 1 TN: 11 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 7 FP: 1 TN: 11 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 8 FP: 1 TN: 11 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 8 FP: 1 TN: 11 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 9 FP: 1 TN: 11 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 11 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 11 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 12 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 12 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 13 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 13 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 13 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 14 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 15 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 15 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 16 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 16 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 16 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 16 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 16 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 16 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 10 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 17 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 18 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 19 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 19 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 19 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 19 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 11 FP: 1 TN: 19 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 12 FP: 1 TN: 19 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 12 FP: 1 TN: 20 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 12 FP: 1 TN: 21 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 12 FP: 1 TN: 21 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 12 FP: 1 TN: 21 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 13 FP: 1 TN: 21 TP: 1\n",
      "accuracy: 0.0\n",
      "FN: 13 FP: 1 TN: 21 TP: 2\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 21 TP: 2\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 21 TP: 2\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 22 TP: 2\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 22 TP: 2\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 22 TP: 3\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 22 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 23 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 23 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 23 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 24 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 24 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 24 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 24 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 24 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 25 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 26 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 27 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 27 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 28 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 28 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 28 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 29 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 30 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 14 FP: 1 TN: 30 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 15 FP: 1 TN: 30 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 15 FP: 1 TN: 31 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 15 FP: 1 TN: 32 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 15 FP: 1 TN: 33 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 15 FP: 1 TN: 33 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 16 FP: 1 TN: 33 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 16 FP: 1 TN: 34 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 16 FP: 1 TN: 34 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 17 FP: 1 TN: 34 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 17 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 18 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 18 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 19 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 19 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 19 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 19 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 35 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 36 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 36 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 36 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 36 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 36 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 37 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 20 FP: 1 TN: 37 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 37 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 37 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 37 TP: 4\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 37 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 38 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 38 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 39 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 39 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 40 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 40 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 41 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 42 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 43 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 44 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 21 FP: 1 TN: 45 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 22 FP: 1 TN: 45 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 22 FP: 1 TN: 45 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 22 FP: 1 TN: 45 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 23 FP: 1 TN: 45 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 23 FP: 1 TN: 45 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 23 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 23 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 23 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 23 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 23 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 24 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 46 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 47 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 47 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 47 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 47 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 47 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 48 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 48 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 48 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 1 TN: 48 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 2 TN: 48 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 2 TN: 48 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 2 TN: 49 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 2 TN: 49 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 25 FP: 2 TN: 50 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 50 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 51 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 51 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 51 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 51 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 51 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 51 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 51 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 52 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 53 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 53 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 53 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 54 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 54 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 55 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 56 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 56 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 26 FP: 2 TN: 57 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 27 FP: 2 TN: 57 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 27 FP: 2 TN: 57 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 27 FP: 2 TN: 58 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 27 FP: 2 TN: 59 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 59 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 59 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 60 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 60 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 61 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 61 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 62 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 62 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 62 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 63 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 64 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 65 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 66 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 67 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 67 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 68 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 69 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 69 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 28 FP: 2 TN: 69 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 69 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 70 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 70 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 70 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 71 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 71 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 71 TP: 5\n",
      "accuracy: 0.0\n",
      "FN: 29 FP: 2 TN: 71 TP: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/unsloth/models/vision.py:205\u001b[0m, in \u001b[0;36munsloth_base_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), autocaster:\n\u001b[0;32m--> 205\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1389\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1371\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     second_per_grid_ts: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m   1388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2_5_VLForConditionalGeneration_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1012\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1208\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1070\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:868\u001b[0m, in \u001b[0;36mQwen2_5_VLSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    859\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# necessary, but kept here for BC\u001b[39;00m\n\u001b[1;32m    867\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2_5_VLSdpaAttention_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:802\u001b[0m, in \u001b[0;36mQwen2_5_VLSdpaAttention_forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    801\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m--> 802\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1744\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1744\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 52\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     45\u001b[0m     proc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     46\u001b[0m     input_text,\n\u001b[1;32m     47\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 52\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/unsloth/models/vision.py:210\u001b[0m, in \u001b[0;36munsloth_base_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_lookup_num_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), autocaster:\n\u001b[0;32m--> 210\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:3211\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3211\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1389\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1371\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     second_per_grid_ts: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m   1388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2_5_VLForConditionalGeneration_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1012\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39madd(delta)\n\u001b[1;32m   1010\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m logits \u001b[38;5;241m=\u001b[39m EMPTY_LOGITS\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1208\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1197\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1198\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         position_embeddings,\n\u001b[1;32m   1206\u001b[0m     )\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1070\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:868\u001b[0m, in \u001b[0;36mQwen2_5_VLSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    859\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# necessary, but kept here for BC\u001b[39;00m\n\u001b[1;32m    867\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2_5_VLSdpaAttention_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:802\u001b[0m, in \u001b[0;36mQwen2_5_VLSdpaAttention_forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    799\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    801\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m--> 802\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    805\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/Linear4bit_peft_forward.py:77\u001b[0m, in \u001b[0;36munsloth_forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_input_dtype(x, lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlora_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dropout, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mIdentity) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/Linear4bit_peft_forward.py:24\u001b[0m, in \u001b[0;36mlora_forward\u001b[0;34m(result, lora_A, lora_B, dropout, x, scaling)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# output = result + scaling * xA @ lora_B.weight.t()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m shape \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_addmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_B\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(shape)\n\u001b[1;32m     32\u001b[0m bias \u001b[38;5;241m=\u001b[39m lora_B\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "FastVisionModel.for_inference(model)\n",
    "acc=0\n",
    "import pickle\n",
    "data_list=[]\n",
    "thinking_pattern = f'{reasoning_start}(.*?){reasoning_end}'\n",
    "answer_pattern = f'{solution_start}(.*?){solution_end}'\n",
    "count=0\n",
    "quant=0\n",
    "FN=0\n",
    "FP=0\n",
    "TP=0\n",
    "TN=0\n",
    "benin_labels = {\n",
    "\"Benign melanocytic proliferations\",\n",
    "\"Benign - Other\",\n",
    "\"Benign epidermal proliferations\",\n",
    "\"Benign soft tissue proliferations - Fibro-histiocytic\",\n",
    "\"Benign soft tissue proliferations - Vascular\",\n",
    "\"Indeterminate epidermal proliferations\",\n",
    "\"Indeterminate melanocytic proliferations\"\n",
    "}\n",
    "malin_labels = {\n",
    "\"Malignant melanocytic proliferations (Melanoma)\",\n",
    "\"Malignant adnexal epithelial proliferations - Follicular\",\n",
    "\"Malignant epidermal proliferations\"\n",
    "}\n",
    "\n",
    "\n",
    "def contains_label(text, labels):\n",
    "    text_lower = text.lower()\n",
    "    return any(label.lower() in text_lower for label in labels)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    \n",
    "    proc = process_sample(test_dataset.iloc[i])\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\":\"text\",\"text\":proc[\"prompt\"][0][\"content\"]}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "        proc[\"image\"],\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            use_cache=True,\n",
    "            temperature=0.5,\n",
    "            min_p=0.1\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    if \"assistant\\n\" in decoded:\n",
    "        prediction_clean = decoded.split(\"assistant\\n\")[-1].strip()\n",
    "    else:\n",
    "        prediction_clean = decoded.strip()\n",
    "        \n",
    "    thinking_matches = re.findall(thinking_pattern, prediction_clean, re.DOTALL)\n",
    "    answer_matches = re.findall(answer_pattern, prediction_clean, re.DOTALL)\n",
    "    \n",
    "    if len(thinking_matches) == 1 and len(answer_matches) == 1:\n",
    "        guess_benin = contains_label(proc[\"answer\"], benin_labels)\n",
    "        guess_malin = contains_label(proc[\"answer\"], malin_labels)\n",
    "        true_benin = contains_label(answer_matches[0], benin_labels)\n",
    "        true_malin = contains_label(answer_matches[0], malin_labels)\n",
    "        \n",
    "        if true_malin or true_benin:\n",
    "            quant += 1\n",
    "            if (true_malin and guess_malin) :\n",
    "                TP += 1\n",
    "            if (true_benin and guess_benin) :\n",
    "                TN +=1\n",
    "            if true_malin and guess_benin:\n",
    "                FN += 1\n",
    "            if true_benin and guess_malin:\n",
    "                FP += 1\n",
    "                \n",
    "    print(\"accuracy:\", count/quant if quant > 0 else 0)\n",
    "    print(\"FN:\", FN,\"FP:\",FP,\"TN:\",TN,\"TP:\",TP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713bbcb-f7a7-45d1-bd68-5b5dc659f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10674e3b-4724-4092-ad7e-7d4736fb72cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: V2_GRPO/ (stored 0%)\n",
      "  adding: V2_GRPO/special_tokens_map.json (deflated 69%)\n",
      "  adding: V2_GRPO/merges.txt (deflated 57%)\n",
      "  adding: V2_GRPO/README.md (deflated 66%)\n",
      "  adding: V2_GRPO/preprocessor_config.json (deflated 50%)\n",
      "  adding: V2_GRPO/tokenizer_config.json (deflated 85%)\n",
      "  adding: V2_GRPO/adapter_model.safetensors (deflated 25%)\n",
      "  adding: V2_GRPO/added_tokens.json (deflated 67%)\n",
      "  adding: V2_GRPO/adapter_config.json (deflated 56%)\n",
      "  adding: V2_GRPO/chat_template.json (deflated 64%)\n",
      "  adding: V2_GRPO/tokenizer.json (deflated 81%)\n",
      "  adding: V2_GRPO/vocab.json (deflated 61%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r best_weights.zip V2_GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee47315-8b5a-4a05-afa8-06e5b0dfa363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id: ISIC_4447861\n",
      "['\\nThe image shows a skin lesion with a brownish color, irregular borders, and some variation in pigmentation. The texture appears slightly raised and uneven. These features are indicative of a potentially malignant lesion.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4448250\n",
      "[\"\\nThe image shows a skin lesion with a flat, slightly raised surface, exhibiting a network-like pattern resembling spider webs. The color is a uniform light pinkish hue, and there are no visible borders or pigmentation changes. The texture appears smooth but slightly rougher than normal skin, which could be due to the fine lines of the pattern.\\n\\nGiven the patient's age and location on the lower extremity, this could be indicative of a benign condition. However, the presence of the network-like pattern might suggest a more complex issue that requires further evaluation.\\n\"]\n",
      "Benign - Other ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4451212\n",
      "[\"\\nThe image shows a close-up of the skin with fine hairs scattered across a light brown background. There are no visible irregularities in the shape, color, or texture that would suggest malignancy. The pattern appears uniform, and there are no signs of inflammation or infection.\\n\\nGiven the patient's age and the location on the upper extremity, this is likely a benign condition.\\n\"]\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4478523\n",
      "[\"\\nThe image shows a small, well-defined, slightly elevated lesion with a darker center and lighter edges. The surrounding skin appears relatively uniform in color, and there are no visible ulcerations or significant asymmetry. The lesion's surface seems smooth without any noticeable scaling or crusting. Based on these observations, the lesion does not exhibit the typical features of a melanoma, which often include irregular borders, variation in color, or asymmetry.\\n\\nGiven the patient's age and the location, this lesion could be considered for a benign lesion. However, the darker central area suggests it might be a benign melanocytic proliferation rather than a simple benign lesion.\\n\"]\n",
      "Benign ['Benign melanocytic proliferations']\n",
      "image_id: ISIC_4495917\n",
      "['\\nThe image shows a small, slightly elevated lesion with a reddish hue and a rough texture. The borders appear somewhat irregular, and there is a slight hyperpigmentation surrounding the lesion. These features are more indicative of a benign condition rather than a malignancy.\\n']\n",
      "Malignant adnexal epithelial proliferations - Follicular ['\\nBenign - Other\\n']\n",
      "image_id: ISIC_4503982\n",
      "['\\nThe image shows a skin lesion with a relatively uniform color and texture. There are no visible irregularities in the border or pattern that would suggest malignancy. The lesion appears to be benign in nature, given its appearance.\\n\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4507702\n",
      "['\\nThe image shows a skin lesion with a slightly elevated surface, uneven borders, and a mix of brown and pinkish hues. The lesion appears to have a slightly irregular texture and a few areas that look like hyperpigmented spots. These features suggest the possibility of a melanocytic proliferation, but further clinical evaluation would be necessary for a definitive diagnosis.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4515981\n",
      "[\"\\nThe image shows a skin lesion with a central area that appears to be a cystic structure, surrounded by a purple-pigmented area. The lesion has irregular borders and a non-uniform texture. There are no clear signs of malignancy such as ulceration, bleeding, or significant asymmetry. However, the presence of a pigmented area and the overall appearance raise concerns.\\n\\nGiven the patient's age and the location on the head/neck, this could be a benign lesion but warrants further evaluation.\\n\"]\n",
      "Benign soft tissue proliferations - Fibro-histiocytic ['Benign - Other']\n",
      "image_id: ISIC_4526369\n",
      "['\\nThe image shows a skin lesion with multiple brown spots of varying sizes and shades. The largest spot has a slightly irregular border, and there appears to be some surrounding hyperpigmentation. The texture looks somewhat rough, and the overall appearance is not uniform.\\n\\nGiven these features, the lesion could be indicative of a melanocytic proliferation due to the presence of pigmented areas and the irregularity of the borders. However, further evaluation through biopsy would be necessary for a definitive diagnosis.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4529433\n",
      "[\"\\nThe image shows a small, raised lesion with a slightly irregular border and a few fine hairs growing through it. The surrounding skin appears relatively normal without any significant discoloration or changes in texture. Given the patient's age and the location on the upper extremity, this could be a benign lesion. However, the presence of fine hairs growing through the lesion could indicate a follicular structure, which is more suggestive of a benign process rather than a malignancy.\\n\"]\n",
      "Benign ['\\nBenign - Other\\n']\n",
      "image_id: ISIC_4533418\n",
      "[\"\\nThe image shows a small, irregularly shaped lesion with a slightly elevated surface. The color is a uniform brownish hue, and there are no visible borders or pigmentation changes that would suggest malignancy. The surrounding skin appears normal without any signs of inflammation or infection.\\n\\nGiven the patient's age and the location, this lesion could be considered benign. However, the irregular shape and slight elevation might warrant further evaluation.\\n\"]\n",
      "Benign ['\\nBenign - Other\\n']\n",
      "image_id: ISIC_4541465\n",
      "['\\nThe image shows a close-up of the skin with fine hair and a slightly raised area. There are no visible color changes, irregular borders, or pigmentation that would suggest malignancy. The texture appears somewhat rough, but this could be due to the natural aging process or other benign conditions.\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4563547\n",
      "[\"\\nThe image shows a skin lesion with a slightly elevated surface, a well-defined border, and a uniform brown color. There are no visible irregularities in the border or color, and the texture appears smooth without any scaling or crusting. Given the patient's age and the location on the upper extremity, this could be a benign lesion.\\n\"]\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4566076\n",
      "['\\nThe image shows a skin lesion with a irregularly shaped, slightly elevated area. The color is dark brown, and there are areas of hyperpigmentation and hypopigmentation. The border appears somewhat irregular, and the texture seems rough. These features are indicative of a melanocytic lesion.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4567446\n",
      "[\"\\nThe image shows a skin lesion with a dark central area surrounded by a lighter halo. The borders appear somewhat irregular, and there is some asymmetry in the lesion's shape. The lesion also exhibits slight elevation and variation in color, which could be indicative of atypical features.\\n\\nGiven these observations, this lesion does not fit the typical characteristics of a benign lesion but rather suggests a possibility of malignancy.\\n\"]\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4571064\n",
      "[\"\\nThe image shows a close-up of skin with a fine, white, linear pattern that resembles a network of capillaries or microvascular structures. There are no visible irregularities in the border, and the color appears uniform. The texture is smooth without any raised areas or nodules.\\n\\nGiven the patient's age and the location on the lower extremity, this could be a benign vascular condition, possibly a port-wine stain or a telangiectasia.\\n\"]\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4586744\n",
      "['\\nThe image shows a skin lesion with a slightly elevated, irregularly shaped area that has a darker pigmentation compared to the surrounding skin. The lesion has a somewhat uneven border and a slightly raised texture. There are also fine hair follicles visible around the lesion.\\n\\nGiven these features, the lesion appears to have characteristics that are more indicative of a malignant melanocytic proliferation rather than a benign one. However, further clinical evaluation including a biopsy would be necessary for a definitive diagnosis.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4589529\n",
      "['\\nThe image shows a well-circumscribed, slightly elevated lesion with a flat top and a whitish center. The border appears slightly irregular but not sharply defined. There are no visible pigmented areas or ulceration. The surrounding skin has a normal appearance without any signs of inflammation.\\n\\nGiven the age of the patient and the location, this lesion could be considered for a benign condition. However, the slight irregularity in the border and the lack of clear pigmentation might raise some concern. A biopsy would be necessary for definitive diagnosis.\\n']\n",
      "Benign ['Benign - Other']\n",
      "image_id: ISIC_4595340\n",
      "[\"\\nThe image shows a skin lesion with a slightly elevated surface, irregular borders, and a mottled coloration ranging from light to dark brown. There are no distinct pigmented structures or areas of ulceration. The texture appears somewhat rough, which could be indicative of a benign lesion but warrants further evaluation for malignancy.\\n\\nGiven the patient's age and the location, this could be a benign lesion like a seborrheic keratosis or actinic keratosis, which are common in older individuals. However, the irregular border and mottled appearance raise concern for a more serious condition.\\n\\n\"]\n",
      "Benign - Other ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4596521\n",
      "['\\nThe image shows a skin lesion with a brownish center and a surrounding area that appears reddish-pink with some scaling. The lesion has an irregular border and a slightly raised texture. There are also some satellite lesions visible around the main lesion.\\n\\nGiven these characteristics, the lesion appears to have features suggestive of a malignant melanocytic proliferation.\\n']\n",
      "Benign melanocytic proliferations ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4602351\n",
      "[\"\\nThe image shows a small, well-circumscribed, slightly elevated lesion with a reddish-brown center and a darker surrounding area. The border appears somewhat irregular but not sharply defined. There is no significant asymmetry or ulceration noted. The lesion's color suggests it might be a nevus or a similar benign lesion.\\n\"]\n",
      "Benign ['Benign melanocytic proliferations']\n",
      "image_id: ISIC_4603933\n",
      "['\\nThe image shows a skin lesion with a mottled appearance, irregular borders, and a central area that appears darker than the surrounding skin. The texture seems slightly raised and uneven. These features are indicative of malignancy, particularly a melanocytic proliferation.\\n\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4614723\n",
      "['\\nThe image shows a skin lesion with a dark central area surrounded by a lighter border. The lesion appears to have uneven borders and a slightly irregular texture. There are no visible satellite lesions or蜊ｫ譏溽羅蜿假ｼ詣hich can be indicative of malignancy. However, the presence of a single lesion with these characteristics warrants further evaluation.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4646199\n",
      "[\"\\nThe image shows a small, dark brown lesion with a slightly raised surface and uneven borders. The surrounding skin appears relatively smooth without significant changes. There are no visible satellite lesions or ulceration. The lesion's appearance is not typical for a benign lesion like a seborrheic keratosis, which usually has a more uniform color and a well-defined, flat surface.\\n\"]\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4656533\n",
      "[\"\\nThe image shows a skin lesion with a well-defined border, slightly irregular but not sharply defined. The central area appears hyperpigmented with a slightly raised texture. There are no visible satellite lesions or ulceration. Given the patient's age and the location on the lower extremity, this could be indicative of a benign lesion.\\n\"]\n",
      "Benign ['Benign - Other']\n",
      "image_id: ISIC_4663947\n",
      "[\"\\nThe image shows a skin lesion with multiple small, raised bumps that appear to be slightly erythematous (red) and have a slightly elevated border. The lesions are scattered across the skin surface, and there are no visible ulcerations or significant changes in color or texture. Given the patient's age and location on the lower extremity, this could be indicative of a benign condition such as a seborrheic keratosis or a verruca vulgaris.\\n\\n\"]\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4681690\n",
      "['\\nThe image shows a small, dark lesion with irregular borders and a slightly raised surface. The color is predominantly dark brown, and there are no visible scales, crusts, or ulcerations. The texture appears rough and uneven.\\n\\nGiven these features, the lesion does not exhibit the typical characteristics of a benign lesion like a seborrheic keratosis, which usually has a smooth, waxy appearance. Additionally, the irregular borders suggest that this could be a melanoma. However, further evaluation with a dermatologist and possibly a biopsy would be necessary for a definitive diagnosis.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4691636\n",
      "['\\nThe image shows a small, irregularly shaped lesion with a dark brown color. The borders appear slightly irregular and uneven. There is no significant elevation or ulceration noted. The texture seems relatively smooth but with some variation in pigmentation. Given these features, the lesion does not appear to be highly suggestive of malignancy but warrants further evaluation.\\n']\n",
      "Benign ['\\nBenign melanocytic proliferations\\n']\n",
      "image_id: ISIC_4694006\n",
      "['\\nThe image shows a close-up view of the skin with numerous fine hair shafts visible. There are no visible changes in color, shape, or texture that would suggest malignancy. The surface appears smooth without any irregularities or borders that might indicate a lesion.\\n']\n",
      "Benign - Other ['\\nBenign - Other\\n']\n",
      "image_id: ISIC_4698636\n",
      "['\\nThe image shows a skin lesion with a brownish color, irregular borders, and some areas of asymmetry. The central part appears darker and more irregular compared to the surrounding area. These features are characteristic of melanoma.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4732929\n",
      "['\\nThe image shows multiple dark spots on the skin. The lesions appear irregular in shape and have uneven borders with some areas showing slight elevation. The color is primarily dark brown to black, and there is variability in the size and distribution of these spots. There are also some lighter areas around the lesions, which could be indicative of atypical melanocytic nevi.\\n']\n",
      "Benign epidermal proliferations ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4733818\n",
      "['\\nThe image shows a skin lesion with a dark, irregularly shaped area surrounded by a lighter skin tone. The borders appear somewhat irregular and the texture within the lesion looks slightly raised and uneven. These features are concerning for malignancy.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4759444\n",
      "['\\nThe image shows a skin lesion with a central hyperpigmented area surrounded by a lighter, more speckled border. There are no significant irregularities in the border, but there are some subtle changes in color and texture. The lesion appears to be relatively small and well-defined.\\n\\nGiven the age of the patient (65) and the location on the head/neck, this could be indicative of a benign lesion. However, the presence of a central hyperpigmented area could be a feature of a melanocytic lesion.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4776217\n",
      "['\\nThe image shows a flat, slightly raised lesion with a few small, yellowish-brown spots scattered across a light-colored background. The border appears relatively smooth but slightly irregular, and the surface has a fine, granular texture. There are no visible ulcerations, crusts, or significant asymmetry that would be typical of a melanoma.\\n\\nGiven these observations, the lesion does not exhibit the characteristics of a melanoma, such as irregular borders, irregular pigmentation, or significant asymmetry. It also lacks the typical features of a benign epidermal proliferation like seborrheic keratosis or actinic keratosis, which often have a more defined border and different coloration.\\n\\nBased on the available information, this lesion appears to be a benign lesion.\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4776301\n",
      "['\\nThe image shows a large number of small, brownish spots scattered across the skin surface. The spots are relatively uniform in size and shape, with a slightly raised texture. There are no significant border irregularities or asymmetry noted. The background skin appears normal without any signs of inflammation or discoloration.\\n\\nGiven these features, the most likely diagnosis is a benign condition. The presence of multiple small, uniform spots suggests a benign epidermal proliferation.\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4783596\n",
      "['\\nThe image shows a skin lesion with a relatively uniform color and texture. There are no significant irregularities in the border or the presence of pigmentation variation that would suggest malignancy. The lesion appears to be benign in nature.\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4807128\n",
      "['\\nThe image shows a small, dark lesion with an irregular border and a slightly elevated surface. The color is predominantly dark brown, and there appears to be some variation in the pigmentation within the lesion. There are no visible satellite lesions or surrounding inflammation.\\n\\nGiven these features, the lesion does not appear to fit the typical criteria for a benign lesion. The presence of an irregular border and variation in pigmentation suggests that this could be a melanoma.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4815153\n",
      "['\\nThe image shows a skin lesion with a relatively uniform color and a slightly raised, slightly irregular border. There are no significant pigmented areas or patterns that would suggest melanoma. The texture appears somewhat rough, which could be indicative of a benign lesion.\\n\\nGiven the patient\\'s age and the location on the upper extremity, the most likely category for this lesion is \"Benign - Other\".\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4815806\n",
      "['\\nThe image shows a skin lesion with a dark brown, irregularly shaped area surrounded by a lighter skin tone. The border appears somewhat uneven and there is some variation in the color within the lesion itself. There are no visible ulceration, bleeding, or significant inflammation that would suggest malignancy.\\n\\nGiven these observations, the lesion does not exhibit the typical characteristics of a benign lesion such as uniform color and regular borders. However, it also lacks the classic features of a melanoma, like a \"ugly duckling\" sign or asymmetry. The irregularity and color variation are more suggestive of a benign lesion rather than a melanoma.\\n']\n",
      "Benign melanocytic proliferations ['Benign - Other']\n",
      "image_id: ISIC_4824083\n",
      "['\\nThe image shows a skin lesion with a dark brown center and lighter surrounding area. The lesion has an uneven border and some asymmetry. The texture appears slightly raised and rough. There are no clear signs of inflammation or infection.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4825698\n",
      "['\\nThe image shows a small, dark, slightly elevated lesion with irregular borders and a rough texture. The surrounding skin appears normal without any significant changes in color or texture.\\n\\nReasoning:\\n- **Shape**: Irregular.\\n- **Color**: Dark, possibly black or very dark brown.\\n- **Border Irregularities**: Uneven and slightly raised.\\n- **Texture**: Rough and uneven.\\n- **Pattern**: No specific pattern observed.\\n\\nGiven these features, the lesion is suggestive of a melanocytic proliferation due to its color, border irregularities, and texture.\\n\\nFinal Diagnosis: Malignant melanocytic proliferations (Melanoma)\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4839937\n",
      "['\\nThe image shows fine, hair-like structures scattered across a light background. There are no distinct borders, colors, or textures that suggest a specific type of lesion. The pattern appears random and does not resemble any typical benign or malignant dermatological conditions.\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_4848137\n",
      "[\"\\nThe image shows a skin lesion with a central dark area, surrounded by a lighter ring, and some erythematous areas. The border appears slightly irregular. There is no significant elevation or ulceration noted. The lesion's characteristics are not typical for a benign lesion but rather suggest a more concerning pattern that could be indicative of malignancy.\\n\"]\n",
      "Malignant epidermal proliferations ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4852619\n",
      "['\\nThe image shows multiple brown spots on the upper extremity of a 70-year-old female. The lesions appear to be flat, with irregular borders and varying sizes. There is no significant elevation or ulceration noted. The color is predominantly brown, which can be indicative of a benign lesion. However, the irregular borders suggest a need for further evaluation.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4864909\n",
      "[\"\\nThe image shows a skin lesion with a well-defined, slightly elevated border, irregularly shaped, and a mottled brownish color. The surface appears somewhat rough, and there are no visible ulcerations or bleeding points. Given the patient's age and location on the lower extremity, this could be a benign lesion, but the irregularities suggest a need for further evaluation.\\n\"]\n",
      "Benign - Other ['\\nBenign melanocytic proliferations\\n']\n",
      "image_id: ISIC_4866668\n",
      "[\"\\nThe image shows a skin lesion with a central brownish area surrounded by fine, hair-like structures. The central area appears slightly elevated and has a rough texture compared to the surrounding skin. There are no significant irregularities in the border or pattern that would suggest malignancy. The lesion's location on the lower extremity and the patient's age are factors to consider but do not change the initial assessment.\\n\\nBased on these observations, the lesion does not exhibit characteristics typical of melanoma, such as irregular borders, asymmetry, or a combination of colors. It also lacks the typical features of a benign epidermal proliferation like a seborrheic keratosis or a warty lesion. The presence of fine hairs around the lesion could indicate a follicular origin, but the central brownish area suggests a different etiology.\\n\"]\n",
      "Benign ['\\nMalignant adnexal epithelial proliferations - Follicular\\n']\n",
      "image_id: ISIC_4869570\n",
      "[\"\\nThe image shows a skin lesion with a rough, uneven surface and a slightly elevated border. The color appears to be a mix of brown and tan, with some areas showing a lighter, almost white hue. There are no clear borders or distinct patterns that would suggest a specific type of benign or malignant proliferation.\\n\\nGiven the patient's age and the location on the upper extremity, this could potentially be a benign lesion. However, the lack of clear borders and the presence of irregularities make it difficult to definitively classify it as benign.\\n\\n\"]\n",
      "Benign - Other ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4885293\n",
      "[\"\\nThe image shows a skin lesion with a rough, uneven surface and some areas that appear reddish-pink. The borders of the lesion are somewhat irregular but not sharply defined. There are no visible ulcerations or crusting, which might suggest malignancy. However, the presence of atypical pigmentation or irregularities could be indicative of a melanocytic lesion.\\n\\nGiven the patient's age and the location on the head/neck, this could be a lesion that requires further evaluation for malignancy.\\n\"]\n",
      "Benign - Other ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4889108\n",
      "['\\nThe image shows a skin lesion with a dark central area surrounded by a lighter, uneven border. The lesion has a slightly raised texture and appears to have irregularities in its shape and color. These features are characteristic of a melanoma.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4898721\n",
      "['\\nThe image shows a large, irregularly shaped lesion with a mottled appearance and some areas that appear hyperpigmented. The borders are somewhat irregular, and there is no visible elevation or ulceration. The texture appears slightly rough, and there are no obvious signs of inflammation or infection.\\n\\nGiven the age of the patient (70) and the location (anterior torso), this could be indicative of a benign process rather than malignancy. However, the irregularity and mottled appearance raise concerns that warrant further investigation.\\n']\n",
      "Benign - Other ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4905924\n",
      "[\"\\nThe image shows a small, round, brown lesion with a slightly elevated border. The color is relatively uniform but has a darker central area. There are no visible ulcerations, significant asymmetry, or other irregularities that would typically suggest malignancy. The lesion appears to be localized and does not exhibit features that are highly suggestive of melanoma.\\n\\nGiven the patient's age and the location on the lower extremity, this could be a benign lesion. However, without a biopsy for histological confirmation, a definitive diagnosis cannot be made.\\n\"]\n",
      "Benign ['Benign - Other']\n",
      "image_id: ISIC_4911837\n",
      "[\"\\nThe image shows a skin lesion with a slightly raised, irregular surface. The color is a uniform tan-brown, and there are no visible borders or patterns that suggest malignancy. The texture appears slightly rough but not ulcerated or crusted, which could be indicative of benign changes.\\n\\nGiven the patient's age and location, this could be a benign lesion. However, the lack of specific features like ulceration, bleeding, or significant asymmetry makes it difficult to definitively rule out malignancy without further clinical information.\\n\"]\n",
      "Benign - Other ['\\nBenign - Other\\n']\n",
      "image_id: ISIC_4957036\n",
      "['\\nThe image shows a skin lesion with a large, irregularly shaped area of discoloration that includes shades of brown and black. The surrounding skin appears inflamed with redness and some scaling. There are no clear borders, and the texture seems rough and uneven. These features are consistent with a suspicious lesion that could be indicative of a melanocytic proliferation.\\n\\n']\n",
      "Indeterminate melanocytic proliferations ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4958714\n",
      "['\\nThe image shows a skin lesion with a few hyperpigmented areas and some irregularities in the surrounding skin. The lesion appears to have a slightly raised border and uneven pigmentation. There are no clear signs of ulceration or significant asymmetry that would be typical for a melanoma. However, the presence of hyperpigmentation and slight irregularities warrant further evaluation.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4971556\n",
      "[\"\\nThe image shows a skin lesion with a relatively uniform brown color, slightly elevated surface, and a well-defined border. The texture appears somewhat granular, which could be indicative of a benign process. However, there are no visible signs of irregularity that would suggest malignancy. Given the patient's age and location on the lower extremity, this lesion could potentially be a benign melanocytic proliferation.\\n\"]\n",
      "Benign - Other ['\\nBenign melanocytic proliferations\\n']\n",
      "image_id: ISIC_4975248\n",
      "['\\nThe image shows a skin lesion with a brownish discoloration that appears slightly elevated and has an irregular border. The texture looks somewhat rough, and there are no clear signs of symmetry or regularity that would suggest a benign lesion. The presence of irregular borders and lack of uniformity suggests a potential for malignancy.\\n']\n",
      "Benign melanocytic proliferations ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_4994871\n",
      "['\\nThe image shows a skin lesion with irregular borders, variation in color from light brown to darker areas, and some irregularly shaped pigmented areas. The texture appears rough, and there is a lack of symmetry which is concerning for malignancy.\\n\\nGiven these features, the lesion is most consistent with a Malignant melanocytic proliferation (Melanoma).\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5042076\n",
      "['\\nThe image shows a skin lesion with a slightly irregular border, uneven pigmentation, and some areas of hyperpigmentation and hypopigmentation. The texture appears somewhat rough, and there are no distinct satellite lesions or ulceration.\\n\\nBased on these features, the lesion does not exhibit the typical characteristics of a benign lesion like a seborrheic keratosis or a senile lentigo. It lacks the uniformity and smoothness expected in those conditions. The presence of irregular borders and variation in pigmentation suggests a higher likelihood of malignancy.\\n']\n",
      "Benign - Other ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5117966\n",
      "[\"\\nThe image shows a small, irregularly shaped lesion with a dark brown center and lighter borders. The lesion has a slightly elevated appearance and appears to have uneven edges. There is no significant bleeding or ulceration visible.\\n\\nGiven the patient's age and the location on the posterior torso, this lesion could be indicative of a melanoma due to the atypical appearance and potential for malignancy.\\n\"]\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5148135\n",
      "['\\nThe image shows a skin lesion with a dark central area surrounded by a lighter, irregularly shaped border. The texture appears rough and there are areas of hyperpigmentation and hypopigmentation. These features are concerning for malignancy.\\n']\n",
      "Indeterminate epidermal proliferations ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5151955\n",
      "['\\nThe image shows a skin lesion with a raised, slightly elevated center, surrounded by a reddish-pink hue. The borders appear slightly irregular and somewhat uneven. There is a central area that looks like a crust or scale, which could be indicative of a chronic inflammatory process. The surrounding skin appears slightly hyperpigmented.\\n\\nGiven these features, the lesion does not exhibit the typical characteristics of a benign melanocytic proliferation (e.g., uniform color, regular borders), nor does it resemble a typical benign epidermal proliferation (e.g., flat, uniform appearance). The presence of irregular borders and the central crust suggests a more complex process, possibly related to inflammation or infection rather than a simple benign growth.\\n\\nFinal Diagnosis: Benign - Other\\n']\n",
      "Benign ['Benign - Other']\n",
      "image_id: ISIC_5158089\n",
      "['\\nThe image shows a skin lesion with a slightly elevated, irregularly shaped area that appears darker than the surrounding skin. The borders of the lesion are somewhat irregular, and there are areas of hyperpigmentation and hypopigmentation within the lesion. The texture appears slightly rough, and there is no visible symmetry.\\n\\nGiven these features, the lesion does not fit the typical characteristics of benign lesions like seborrheic keratoses or actinic keratoses. The irregular shape, asymmetry, and variation in color suggest a more complex pathology.\\n\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5161902\n",
      "[\"\\nThe image shows a small, dark brown lesion with a slightly elevated surface on a background of erythematous skin. The lesion has a well-defined border and a smooth surface. There are no visible ulcerations, bleeding, or significant asymmetry that would suggest malignancy. However, the presence of a single lesion without a clear history or additional lesions makes it difficult to definitively rule out melanoma.\\n\\nGiven the patient's age and the location, this could be a benign lesion, but the lack of complete clinical information makes it challenging to provide a definitive diagnosis.\\n\"]\n",
      "Benign ['Benign - Other']\n",
      "image_id: ISIC_5167099\n",
      "['\\nThe image shows a close-up of a skin lesion with a fine, regular pattern resembling goosebumps or tiny hairs. There are no visible irregularities in the border, and the color appears uniform. The texture is consistent without any signs of inflammation or discoloration.\\n\\nGiven these observations:\\n- The lesion does not exhibit the typical characteristics of melanoma (asymmetry, irregular borders, variation in color, etc.).\\n- There are no signs of inflammation or infection.\\n- The pattern resembles follicular hyperkeratosis, which is a benign condition often seen in older individuals.\\n\\nFinal Diagnosis: Benign - Other\\n']\n",
      "Benign - Other ['Benign - Other']\n",
      "image_id: ISIC_5188015\n",
      "['\\nThe image shows a small, well-circumscribed lesion with a slightly elevated surface. The color is predominantly brownish with some variation in shade, and the borders appear somewhat irregular but not sharply defined. There are no visible ulceration or significant bleeding points. The texture appears relatively uniform without any obvious papules or nodules. Given the age and location, this could be a benign lesion, possibly a seborrheic keratosis or a benign pigmented lesion.\\n']\n",
      "Benign ['\\nBenign - Other\\n']\n",
      "image_id: ISIC_5199953\n",
      "['\\nThe image shows a large, irregularly shaped lesion with a rough texture and a mottled coloration ranging from light brown to tan. The borders appear uneven and somewhat irregular, and there are areas with hyperpigmentation and hypopigmentation. These features are consistent with the characteristics of a malignant melanocytic proliferation.\\n']\n",
      "Benign - Other ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5210866\n",
      "['\\nThe image shows a skin lesion with a dark central area surrounded by a lighter, irregularly shaped border. The texture appears somewhat granular, and there is no significant elevation or ulceration. The lesion is located on the lower extremity of a 85-year-old male.\\n\\nGiven these characteristics, the lesion does not exhibit the typical features of a benign lesion such as uniform color, smooth borders, and regular symmetry. Instead, the presence of a darker central area and irregular borders are more indicative of malignancy.\\n']\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5214005\n",
      "[\"\\nThe image shows a skin lesion with a distinct brown spot surrounded by a lighter area. The lesion has an uneven border and a slightly elevated texture. There are no visible ulcerations or significant asymmetry that would be typical of melanoma.\\n\\nGiven the patient's age and the appearance of the lesion, this could be a benign lesion. However, the irregular border and slight elevation suggest it might not be entirely benign.\\n\\n\"]\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5230609\n",
      "[\"\\nThe image shows a small, irregularly shaped lesion with a brownish color. The borders appear slightly uneven, and there is some surrounding hyperpigmentation. However, no significant ulceration or bleeding is observed. The lesion's texture seems relatively smooth but lacks the uniformity typically associated with benign lesions.\\n\"]\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n",
      "image_id: ISIC_5233298\n",
      "[\"\\nThe image shows a skin lesion with a slightly elevated, irregularly shaped area. The central part appears darker compared to the surrounding skin, which has a lighter brownish hue. The borders of the lesion are somewhat uneven and lack symmetry, suggesting possible irregularity. The texture appears slightly rougher than the surrounding skin. There are no visible scales, crusts, or ulcerations.\\n\\nGiven these features, the lesion does not appear to be a benign epidermal proliferation, as it lacks the typical smooth and uniform appearance. It also does not resemble a benign soft tissue proliferation, as there are no signs of tissue overgrowth or inflammation. The lesion's irregular shape and color variation suggest the possibility of malignancy.\\n\\n\"]\n",
      "Benign ['Malignant melanocytic proliferations (Melanoma)']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/unsloth/models/vision.py:205\u001b[0m, in \u001b[0;36munsloth_base_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), autocaster:\n\u001b[0;32m--> 205\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1389\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1371\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     second_per_grid_ts: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m   1388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2_5_VLForConditionalGeneration_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1012\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1208\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1067\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m-> 1067\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:173\u001b[0m, in \u001b[0;36mQwen2RMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2RMSNorm_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:155\u001b[0m, in \u001b[0;36mQwen2RMSNorm_forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Qwen2_5_VisionPatchEmbed_forward(\u001b[38;5;28mself\u001b[39m, hidden_states)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(fullgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, dynamic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, options \u001b[38;5;241m=\u001b[39m torch_compile_options)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mQwen2RMSNorm_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    157\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1184\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1183\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:323\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    322\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 323\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m     out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:672\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    670\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 672\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:490\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_inductor/output_code.py:466\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_inductor/utils.py:2128\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m-> 2128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_ec2-user/py/cpymkjcnumzn3usmgqnyjqdrpsmwosqyjlam62tfzgd4fiz2olu6.py:126\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    125\u001b[0m assert_size_stride(arg1_1, (\u001b[38;5;241m3584\u001b[39m, ), (\u001b[38;5;241m1\u001b[39m, ))\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_DeviceGuard(\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    127\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:425\u001b[0m, in \u001b[0;36m_DeviceGuard.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m: Any, value: Any, traceback: Any):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_exchange_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 54\u001b[0m\n\u001b[1;32m     46\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     47\u001b[0m     proc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     48\u001b[0m     input_text,\n\u001b[1;32m     49\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 54\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/unsloth/models/vision.py:210\u001b[0m, in \u001b[0;36munsloth_base_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_lookup_num_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), autocaster:\n\u001b[0;32m--> 210\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:3211\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3211\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1389\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1371\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     second_per_grid_ts: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs,\n\u001b[1;32m   1388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2_5_VLForConditionalGeneration_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:1012\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39madd(delta)\n\u001b[1;32m   1010\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m logits \u001b[38;5;241m=\u001b[39m EMPTY_LOGITS\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1208\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1197\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1198\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         position_embeddings,\n\u001b[1;32m   1206\u001b[0m     )\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1070\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:868\u001b[0m, in \u001b[0;36mQwen2_5_VLSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    859\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# necessary, but kept here for BC\u001b[39;00m\n\u001b[1;32m    867\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwen2_5_VLSdpaAttention_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/unsloth_compiled_module_qwen2_5_vl.py:803\u001b[0m, in \u001b[0;36mQwen2_5_VLSdpaAttention_forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    801\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    802\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 803\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    806\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/Linear4bit_peft_forward.py:77\u001b[0m, in \u001b[0;36munsloth_forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_input_dtype(x, lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlora_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dropout, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mIdentity) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/SageMaker/unsloth_compiled_cache/Linear4bit_peft_forward.py:24\u001b[0m, in \u001b[0;36mlora_forward\u001b[0;34m(result, lora_A, lora_B, dropout, x, scaling)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# output = result + scaling * xA @ lora_B.weight.t()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m shape \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_addmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_B\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(shape)\n\u001b[1;32m     32\u001b[0m bias \u001b[38;5;241m=\u001b[39m lora_B\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "FastVisionModel.for_inference(model)\n",
    "acc=0\n",
    "import pickle\n",
    "data_list=[]\n",
    "thinking_pattern = f'{reasoning_start}(.*?){reasoning_end}'\n",
    "answer_pattern = f'{solution_start}(.*?){solution_end}'\n",
    "count=0\n",
    "quant=0\n",
    "FN=0\n",
    "FP=0\n",
    "TP=0\n",
    "TN=0\n",
    "benin_labels = {\n",
    "\"Benign melanocytic proliferations\",\n",
    "\"Benign - Other\",\n",
    "\"Benign epidermal proliferations\",\n",
    "\"Benign soft tissue proliferations - Fibro-histiocytic\",\n",
    "\"Benign soft tissue proliferations - Vascular\",\n",
    "\"Indeterminate epidermal proliferations\",\n",
    "\"Indeterminate melanocytic proliferations\"\n",
    "}\n",
    "malin_labels = {\n",
    "\"Malignant melanocytic proliferations (Melanoma)\",\n",
    "\"Malignant adnexal epithelial proliferations - Follicular\",\n",
    "\"Malignant epidermal proliferations\"\n",
    "}\n",
    "\n",
    "\n",
    "def contains_label(text, labels):\n",
    "    text_lower = text.lower()\n",
    "    return any(label.lower() in text_lower for label in labels)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    \n",
    "    proc = process_sample(test_dataset.iloc[i])\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\":\"text\",\"text\":proc[\"prompt\"][0][\"content\"]}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "        proc[\"image\"],\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            use_cache=True,\n",
    "            temperature=0.5,\n",
    "            min_p=0.1\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    if \"assistant\\n\" in decoded:\n",
    "        prediction_clean = decoded.split(\"assistant\\n\")[-1].strip()\n",
    "    else:\n",
    "        prediction_clean = decoded.strip()\n",
    "        \n",
    "    thinking_matches = re.findall(thinking_pattern, prediction_clean, re.DOTALL)\n",
    "    answer_matches = re.findall(answer_pattern, prediction_clean, re.DOTALL)\n",
    "    \n",
    "    print(\"image_id:\",proc[\"image_id\"])\n",
    "    print(thinking_matches)\n",
    "    print(proc[\"answer\"],answer_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747eac8-154d-4226-8f19-11ddb4c671c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
